{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from DataLoader import get_dataloader\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_file, transform=None, cache_file='valid_indices_cache.pkl'):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.cache_file = cache_file\n",
    "        with open(label_file, 'r', encoding='utf-8') as f:\n",
    "            self.labels = f.readlines()\n",
    "        self.valid_indices = self._load_or_create_valid_indices()\n",
    "\n",
    "    def _load_or_create_valid_indices(self):\n",
    "        if os.path.exists(self.cache_file):\n",
    "            with open(self.cache_file, 'rb') as f:\n",
    "                valid_indices = pickle.load(f)\n",
    "        else:\n",
    "            valid_indices = self._get_valid_indices()\n",
    "            with open(self.cache_file, 'wb') as f:\n",
    "                pickle.dump(valid_indices, f)\n",
    "        return valid_indices\n",
    "\n",
    "    def _get_valid_indices(self):\n",
    "        valid_indices = []\n",
    "        for idx in range(len(self.labels)):\n",
    "            img_name = os.path.join(self.image_dir, f\"{idx:07d}.png\")\n",
    "            if os.path.exists(img_name):\n",
    "                valid_indices.append(idx)\n",
    "        return valid_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        img_name = os.path.join(self.image_dir, f\"{actual_idx:07d}.png\")\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = self.labels[actual_idx].strip()\n",
    "        image = self.resize_and_pad(image,800,400)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "            image = transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "    def resize_and_pad(self, image, target_width, target_height):\n",
    "        # Calculate the ratio to maintain the aspect ratio\n",
    "        original_width, original_height = image.size\n",
    "        ratio = min(target_width / original_width, target_height / original_height)\n",
    "    \n",
    "        # Resize the image while maintaining the aspect ratio\n",
    "        new_size = (int(original_width * ratio), int(original_height * ratio))\n",
    "        resized_image = image.resize(new_size, Image.LANCZOS)\n",
    "    \n",
    "        # Create a new image with the specified target size and a white background\n",
    "        new_image = Image.new(\"RGB\", (target_width, target_height), (255, 255, 255))\n",
    "    \n",
    "        # Calculate the position to paste the resized image on the white background\n",
    "        paste_x = (target_width - new_size[0]) // 2\n",
    "        paste_y = (target_height - new_size[1]) // 2\n",
    "    \n",
    "        # Paste the resized image onto the white background\n",
    "        new_image.paste(resized_image, (paste_x, paste_y))\n",
    "    \n",
    "        return new_image\n",
    "\n",
    "def get_dataloader(batch_size, image_dir='../../UniMER-1M/images/', label_file='../../UniMER-1M/train.txt', transform=None, cache_file='valid_indices_cache.pkl'):\n",
    "    dataset = CustomDataset(image_dir=image_dir, label_file=label_file, transform=transform, cache_file=cache_file)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding2D(nn.Module):\n",
    "    def __init__(self, d_model, height, width):\n",
    "        super().__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.d_model = d_model\n",
    "        self.pe: torch.Tensor = self._get_positional_encoding(d_model, height, width)\n",
    "\n",
    "    def _get_positional_encoding(self, d_model, height, width):\n",
    "        \"\"\"\n",
    "        :param d_model: dimension of the model\n",
    "        :param height: height of the positions\n",
    "        :param width: width of the positions\n",
    "        :return: d_model*height*width position matrix\n",
    "        \"\"\"\n",
    "        if d_model % 4 != 0:\n",
    "            raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
    "                            \"odd dimension (got dim={:d})\".format(d_model))\n",
    "        pe = torch.zeros(d_model, height, width)\n",
    "        # Each dimension use half of d_model\n",
    "        d_model = int(d_model / 2)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) *\n",
    "                            -(math.log(10000.0) / d_model))\n",
    "        pos_w = torch.arange(0., width).unsqueeze(1)\n",
    "        pos_h = torch.arange(0., height).unsqueeze(1)\n",
    "        pe[0:d_model:2, :, :] = torch.sin(pos_w * div_term).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n",
    "        pe[1:d_model:2, :, :] = torch.cos(pos_w * div_term).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n",
    "        pe[d_model::2, :, :] = torch.sin(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
    "        pe[d_model + 1::2, :, :] = torch.cos(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
    "\n",
    "        return pe\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, channels, height, width)\n",
    "        Returns:\n",
    "            Tensor with positional encodings added, of shape (batch_size, channels, height, width)\n",
    "        \"\"\"\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        # Ensure the input has the correct number of channels\n",
    "        assert self.d_model == channels, \"Dimension mismatch: d_model and input channels must be the same\"\n",
    "        # Add positional encodings to the input tensor\n",
    "        x = x + self.pe.unsqueeze(0) #the unsqueeze() might not be necessary, idk\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_dataloader(batch_size=10)\n",
    "\n",
    "# Assuming the DataLoader for training is defined as train_loader\n",
    "i = 0\n",
    "for images, labels in train_loader:\n",
    "    pil_image = transforms.ToPILImage()(images[0]).convert(\"RGB\")\n",
    "    # pil_image.show()\n",
    "    # print(labels[0])    \n",
    "    plt.imshow(images[0].permute(1, 2, 0))\n",
    "    \n",
    "    plt.title(f\"Label: {labels[0]}\")\n",
    "    plt.show()\n",
    "    if i > 10:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swastikagrawal/Library/Python/3.9/lib/python/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/swastikagrawal/Library/Python/3.9/lib/python/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet169_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet169_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1664 25\n",
      "tensor([[[[-4.2097e-03,  8.4160e-01,  9.0742e-01,  ...,  4.1541e-01,\n",
      "           -5.4218e-01, -9.9898e-01],\n",
      "          [-6.5510e-04,  8.4362e-01,  9.1039e-01,  ...,  4.1167e-01,\n",
      "           -5.4177e-01, -9.9840e-01],\n",
      "          [-7.3636e-04,  8.4410e-01,  9.1004e-01,  ...,  4.1122e-01,\n",
      "           -5.4156e-01, -9.9990e-01],\n",
      "          ...,\n",
      "          [ 1.1227e-03,  8.4144e-01,  9.0773e-01,  ...,  4.1352e-01,\n",
      "           -5.4382e-01, -9.9985e-01],\n",
      "          [-1.8630e-03,  8.3972e-01,  9.0952e-01,  ...,  4.0974e-01,\n",
      "           -5.4319e-01, -1.0022e+00],\n",
      "          [-5.6795e-03,  8.4561e-01,  9.0938e-01,  ...,  4.1248e-01,\n",
      "           -5.4239e-01, -1.0001e+00]],\n",
      "\n",
      "         [[ 1.0001e+00,  5.4033e-01, -4.1668e-01,  ..., -9.1465e-01,\n",
      "           -8.4142e-01,  5.4097e-03],\n",
      "          [ 1.0010e+00,  5.3968e-01, -4.1372e-01,  ..., -9.1030e-01,\n",
      "           -8.3687e-01,  2.5407e-03],\n",
      "          [ 9.9948e-01,  5.4101e-01, -4.1600e-01,  ..., -9.0913e-01,\n",
      "           -8.3588e-01,  4.4913e-03],\n",
      "          ...,\n",
      "          [ 9.9767e-01,  5.4022e-01, -4.1339e-01,  ..., -9.0874e-01,\n",
      "           -8.4063e-01,  4.6926e-03],\n",
      "          [ 1.0004e+00,  5.4020e-01, -4.1580e-01,  ..., -9.1143e-01,\n",
      "           -8.4069e-01,  4.4613e-03],\n",
      "          [ 9.9894e-01,  5.4012e-01, -4.1638e-01,  ..., -9.1148e-01,\n",
      "           -8.3864e-01,  4.2688e-03]],\n",
      "\n",
      "         [[-1.8392e-03,  8.3023e-01,  9.2656e-01,  ...,  5.8320e-01,\n",
      "           -3.4829e-01, -9.7311e-01],\n",
      "          [ 4.3348e-05,  8.2939e-01,  9.2681e-01,  ...,  5.8273e-01,\n",
      "           -3.4890e-01, -9.7248e-01],\n",
      "          [-2.9435e-04,  8.2913e-01,  9.2673e-01,  ...,  5.8268e-01,\n",
      "           -3.4885e-01, -9.7216e-01],\n",
      "          ...,\n",
      "          [ 8.6991e-05,  8.2952e-01,  9.2624e-01,  ...,  5.8293e-01,\n",
      "           -3.4863e-01, -9.7243e-01],\n",
      "          [-1.2096e-04,  8.2908e-01,  9.2654e-01,  ...,  5.8282e-01,\n",
      "           -3.4830e-01, -9.7242e-01],\n",
      "          [-1.8258e-03,  8.2926e-01,  9.2663e-01,  ...,  5.8262e-01,\n",
      "           -3.4865e-01, -9.7225e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3529e+00,  5.5306e+00,  6.9269e+00,  ..., -1.2974e-01,\n",
      "            9.3444e-01,  1.3843e+00],\n",
      "          [ 4.6389e+00,  6.3216e+00,  7.6605e+00,  ..., -2.1886e-01,\n",
      "            2.4333e+00,  2.6541e+00],\n",
      "          [ 2.7507e+00,  4.2366e+00,  7.1466e+00,  ...,  4.9108e-01,\n",
      "            7.0239e-01,  8.8992e-01],\n",
      "          ...,\n",
      "          [ 6.8517e-01,  3.3655e-01,  5.6977e-01,  ...,  1.3085e+00,\n",
      "            1.3386e+00,  1.8495e+00],\n",
      "          [-6.1549e-01, -2.9910e-01,  5.1747e-01,  ...,  1.2672e-01,\n",
      "            1.3746e-01,  7.2094e-01],\n",
      "          [-2.6360e+00, -5.6189e-01,  5.7584e-01,  ...,  1.9027e-01,\n",
      "           -9.7936e-01, -4.6316e-01]],\n",
      "\n",
      "         [[-1.5147e+00, -1.4091e+00, -4.7327e-01,  ..., -6.3431e-01,\n",
      "            8.2745e-01,  8.7399e-01],\n",
      "          [-1.7889e+00, -1.5129e+00, -6.9895e-01,  ..., -3.6511e-01,\n",
      "           -2.7621e-01, -6.7359e-02],\n",
      "          [ 4.2081e-01, -5.0209e-01, -7.5507e-01,  ...,  3.3853e-03,\n",
      "            7.4055e-01,  1.0302e+00],\n",
      "          ...,\n",
      "          [ 1.5343e-01,  9.3279e-02, -1.2725e+00,  ...,  3.7304e+00,\n",
      "            6.1298e+00,  6.7352e+00],\n",
      "          [-4.7532e-01,  1.0961e+00,  3.8412e-01,  ...,  5.0292e+00,\n",
      "            5.0335e+00,  6.2892e+00],\n",
      "          [-2.2507e+00, -2.1198e-01,  4.3822e-01,  ...,  4.0287e+00,\n",
      "            4.4676e+00,  5.5879e+00]],\n",
      "\n",
      "         [[-1.1342e+00, -5.6768e-01, -4.8910e-01,  ...,  9.2231e-01,\n",
      "            1.3984e+00,  1.7856e+00],\n",
      "          [ 4.6946e-02, -9.7821e-01, -4.6047e-01,  ...,  1.3781e+00,\n",
      "            2.1221e+00,  1.5045e+00],\n",
      "          [ 3.2416e-01, -8.0950e-01, -9.2959e-02,  ...,  2.2977e+00,\n",
      "            6.8810e+00,  6.3590e+00],\n",
      "          ...,\n",
      "          [-1.2269e-01, -1.1204e-01, -1.3960e-02,  ...,  8.9387e-01,\n",
      "           -1.1122e-02,  2.8594e-01],\n",
      "          [-2.3649e+00, -1.9605e+00,  4.2707e-01,  ...,  3.9175e-01,\n",
      "           -3.2327e-01, -1.1627e-02],\n",
      "          [-3.1517e+00, -2.5711e+00,  6.5004e-01,  ...,  1.3404e+00,\n",
      "            3.2820e-01,  6.7299e-01]]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = models.densenet169(pretrained=True)\n",
    "# Remove the final fully connected layer to get the final feature maps\n",
    "model = nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "#to get the size of the output of the denseNet\n",
    "image_tensor = torch.randn(1, 3, 800, 400)\n",
    "output = model(image_tensor)\n",
    "size = output.size()\n",
    "\n",
    "model.add_module('PositionalEncoding2D', PositionalEncoding2D(1664, size[2], size[3])) # hardcoded this based on denseNet output size\n",
    "#yet to automate that, the 7x7 is dependent on the input size of the model, it can be anything. Still need to automate this\n",
    "image_tensor = torch.randn(1, 3, 800, 400)\n",
    "print(model(image_tensor))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
